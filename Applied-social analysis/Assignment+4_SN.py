
# coding: utf-8

# ---
# 
# _You are currently looking at **version 1.2** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-social-network-analysis/resources/yPcBs) course resource._
# 
# ---

# # Assignment 4

# In[2]:


import networkx as nx
import pandas as pd
import numpy as np
import pickle


# ---
# 
# ## Part 1 - Random Graph Identification
# 
# For the first part of this assignment you will analyze randomly generated graphs and determine which algorithm created them.

# In[3]:


P1_Graphs = pickle.load(open('A4_graphs','rb'))
P1_Graphs


# In[4]:


def degree_distribution():
    fa=[]
    for i in range (0,5):
        degrees=P1_Graphs[i].degree()
        degree_values=sorted(degrees.values())
        #set_degree_values=sorted(set(degrees.values()))
        #fa.append((sum(degree_values)/len(degree_values),nx.average_clustering(P1_Graphs[i]),nx.average_shortest_path_length(P1_Graphs[i]))) #average shortest path length
        new={}
        for j in degree_values:
            if j in new:
                new[j]=new[j]+1
            else:
                new[j]=1
        nod=nx.number_of_nodes(P1_Graphs[i])
        neww=[]
        for i in new.items():
            neww.append((i[0],i[1]/nod))
        fa.append((len(neww),neww))
        #print(nod)
    return fa
    


# In[5]:


degrees=P1_Graphs[1].degree()
degree_values=sorted(degrees.values())
max(degree_values)


# In[6]:


degree_distribution()


# <br>
# `P1_Graphs` is a list containing 5 networkx graphs. Each of these graphs were generated by one of three possible algorithms:
# * Preferential Attachment (`'PA'`)
# * Small World with low probability of rewiring (`'SW_L'`)
# * Small World with high probability of rewiring (`'SW_H'`)
# 
# Anaylze each of the 5 graphs and determine which of the three algorithms generated the graph.
# 
# *The `graph_identification` function should return a list of length 5 where each element in the list is either `'PA'`, `'SW_L'`, or `'SW_H'`.*

# In[ ]:





# In[7]:



def graph_identification():
    G1=P1_Graphs[1]
    acc1=nx.average_clustering(G1)
    sp1=nx.average_shortest_path_length(G1)
    fa=[]
    fa=['PA','SW_L','SW_L','PA','SW_H'] # IN 5th graph acc reduces due to high prob ( also it does not follow PDL) 
    # Your Code Here
    #Note : If prob of rewiring is high, then all nodes approx. degree distribution is almost same for all nodes , but this was not the case with second graph and was satisfied with fifth graph.
    
    return fa# Your Answer Here


# In[8]:


graph_identification()


# ---
# 
# ## Part 2 - Company Emails
# 
# For the second part of this assignment you will be workking with a company's email network where each node corresponds to a person at the company, and each edge indicates that at least one email has been sent between two people.
# 
# The network also contains the node attributes `Department` and `ManagementSalary`.
# 
# `Department` indicates the department in the company which the person belongs to, and `ManagementSalary` indicates whether that person is receiving a management position salary.

# In[9]:


G = nx.read_gpickle('email_prediction.txt')

print(nx.info(G))


# ### Part 2A - Salary Prediction
# 
# Using network `G`, identify the people in the network with missing values for the node attribute `ManagementSalary` and predict whether or not these individuals are receiving a management position salary.
# 
# To accomplish this, you will need to create a matrix of node features using networkx, train a sklearn classifier on nodes that have `ManagementSalary` data, and predict a probability of the node receiving a management salary for nodes where `ManagementSalary` is missing.
# 
# 
# 
# Your predictions will need to be given as the probability that the corresponding employee is receiving a management position salary.
# 
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
# 
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).
# 
# Using your trained classifier, return a series of length 252 with the data being the probability of receiving management salary, and the index being the node id.
# 
#     Example:
#     
#         1       1.0
#         2       0.0
#         5       0.8
#         8       1.0
#             ...
#         996     0.7
#         1000    0.5
#         1001    0.0
#         Length: 252, dtype: float64

# In[10]:


def salary_predictions():
    ans=nx.get_node_attributes(G,"ManagementSalary")
    ans2=nx.get_node_attributes(G,"Department")
    
    cl3=ans.values()
    temp=G.edges(data=True)
    cl1=[]
    cl2=[]
    for i in temp:
        cl1.append(i[0])
        cl2.append(i[1])
    df=pd.DataFrame()
    
    df['all']=[i for i in range(0,1005)]
    df['department']=ans2.values()
    df['degree']=pd.Series(nx.degree(G))
    df['degree_centrality']=pd.Series(nx.degree_centrality(G))
    df['pagerank']=pd.Series(nx.pagerank(G))
    df['betweenness']=pd.Series(nx.betweenness_centrality(G))
    df['cluster']=pd.Series(nx.clustering(G))
    df['salary']=cl3
    df['salary'].fillna(2,inplace=True)
    train_df=df[ df['salary']==0 ]
    temp_df=df [ df['salary']==1 ]
    f_df=pd.concat([train_df,temp_df],ignore_index=True)
    test_df=df [df['salary']==2]
    test_df=test_df.drop(['all','department'],axis=1)
    f_df=f_df.drop(['all','department'],axis=1)
    y_train=f_df['salary']
    X_train=f_df.drop('salary',axis=1)
    test_df=test_df.drop('salary',axis=1)
    from sklearn.naive_bayes import GaussianNB
    from sklearn.svm import SVC
    from sklearn.metrics import roc_auc_score
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.model_selection import GridSearchCV
    clf=GradientBoostingClassifier(random_state=0)
    values = {'learning_rate': [0.01, 0.1, 1], 'max_depth': [3, 4, 5]}
    model=GridSearchCV(clf,param_grid=values,scoring='roc_auc')
    model.fit(X_train,y_train)
    ans=model.predict_proba(test_df)[:,1]
    final_answer=pd.Series(ans,test_df.index)
    
    # Your Code Here
    
    return final_answer# AUC of 0.89


# In[11]:


salary_predictions()


# ### Part 2B - New Connections Prediction
# 
# For the last part of this assignment, you will predict future connections between employees of the network. The future connections information has been loaded into the variable `future_connections`. The index is a tuple indicating a pair of nodes that currently do not have a connection, and the `Future Connection` column indicates if an edge between those two nodes will exist in the future, where a value of 1.0 indicates a future connection.

# In[12]:


future_connections = pd.read_csv('Future_Connections.csv', index_col=0, converters={0: eval})
future_connections.head(10)


# Using network `G` and `future_connections`, identify the edges in `future_connections` with missing values and predict whether or not these edges will have a future connection.
# 
# To accomplish this, you will need to create a matrix of features for the edges found in `future_connections` using networkx, train a sklearn classifier on those edges in `future_connections` that have `Future Connection` data, and predict a probability of the edge being a future connection for those edges in `future_connections` where `Future Connection` is missing.
# 
# 
# 
# Your predictions will need to be given as the probability of the corresponding edge being a future connection.
# 
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
# 
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).
# 
# Using your trained classifier, return a series of length 122112 with the data being the probability of the edge being a future connection, and the index being the edge as represented by a tuple of nodes.
# 
#     Example:
#     
#         (107, 348)    0.35
#         (542, 751)    0.40
#         (20, 426)     0.55
#         (50, 989)     0.35
#                   ...
#         (939, 940)    0.15
#         (555, 905)    0.35
#         (75, 101)     0.65
#         Length: 122112, dtype: float64

# In[13]:


jc=[] 
cn=[]
rai=[]
pa=[]
sh=[]


# In[14]:


df=future_connections
df['pairs']=future_connections.index
df['Future Connection']=df['Future Connection'].fillna(2)


# In[15]:


for node in G.nodes():
    G.node[node]['community']=G.node[node]['Department']


# In[16]:


for i in range(0,len(df)):
    cn.append(len(list(nx.common_neighbors(G,df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1]))))
    


# In[18]:


for i in range(0,len(df)):
    jc.append(list(nx.jaccard_coefficient(G,[(df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1])]))[0][2])


# In[22]:


for i in range(0,len(df)):
    rai.append(list(nx.resource_allocation_index(G,[(df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1])]))[0][2])
    


# In[23]:


for i in range(0,len(df)):
    pa.append(list(nx.preferential_attachment(G,[(df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1])]))[0][2])
    


# In[24]:


for i in range(0,len(df)):
    sh.append(list(nx.cn_soundarajan_hopcroft(G,[(df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1])]))[0][2])
    


# In[25]:


df['jacc']=jc
df['common']=cn
df['reso']=rai
df['prefer']=pa
df['sound']=sh


# In[30]:


X_temp1=df[ df['Future Connection']==0]
X_temp2=df[ df['Future Connection']==1]
X_temp=pd.concat([X_temp1,X_temp2],axis=0)
y_train=X_temp['Future Connection']
X_train=X_temp.drop('Future Connection',axis=1)
X_test=df[df['Future Connection']==2]
X_test=X_test.drop('Future Connection',axis=1)


# In[32]:


X_train=X_train.drop('pairs',axis=1)
X_test=X_test.drop('pairs',axis=1)


# In[38]:


from sklearn.metrics import roc_auc_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
clf=GradientBoostingClassifier(random_state=0)
values = {'learning_rate': [0.01, 0.1, 1], 'max_depth': [3, 4, 5]}
model=GridSearchCV(clf,param_grid=values,scoring='roc_auc')
model.fit(X_train,y_train)
ans=model.predict_proba(X_test)[:,1]
final_answer=pd.Series(ans,X_test.index)


# In[40]:


def new_connections_predictions():
    
    """
    df=future_connections
    df['pairs']=future_connections.index
    df['Future Connection']=df['Future Connection'].fillna(2)
    
    #l=list
    #temp=nx.jaccard_coefficient(G,[(6,840)])
    #l.append(temp)
    jc=[] 
    cn=[]
    rai=[]
    pa=[]
    sh=[]
    for node in G.nodes():
        G.node[node]['community']=G.node[node]['Department']
    for i in range(0,len(df)):
        #cn.append(len(list(nx.common_neighbors(G,df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1]))))
        jc.append(list(nx.jaccard_coefficient(G,[(df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1])]))[0][2])
        cn.append(len(list(nx.common_neighbors(G,df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1]))))
        #jc.append(list(nx.jaccard_coefficient(G,[(df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1])])))
        rai.append(list(nx.resource_allocation_index(G,[(df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1])]))[0][2])
        pa.append(list(nx.preferential_attachment(G,[(df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1])]))[0][2])
        sh.append(list(nx.cn_soundarajan_hopcroft(G,[(df.iloc[i]['pairs'][0],df.iloc[i]['pairs'][1])]))[0][2])
    df['jacc']=jc
    df['common']=cn
    df['reso']=rai
    df['prefer']=pa
    df['sound']=sh
    X_temp1=df[ df['Future Connection']==0]
    X_temp2=df[ df['Future Connection']==1]
    X_temp=pd.concat(X_temp1,X_temp2,axis=0)
    y_train=X_temp['Future Connection']
    X_train=X_train.drop('Future Connection',axis=1)
    X_test=df[df['Future Connection']==2]
    X_test=X_test.dropna('Future Connection',axis=1)
    from sklearn.metrics import roc_auc_score
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.model_selection import GridSearchCV
    clf=GradientBoostingClassifier(random_state=0)
    values = {'learning_rate': [0.01, 0.1, 1], 'max_depth': [3, 4, 5]}
    model=GridSearchCV(clf,param_grid=values,scoring='roc_auc')
    model.fit(X_train,y_train)
    ans=model.predict_proba(X_test)[:,1]
    final_answer=pd.Series(ans,X_test.index)
    """
    
    
        
        
    
    
    return final_answer#list(nx.jaccard_coefficient(G,[(6,840)]))#(nx.jaccard_coefficient(G,[(df.iloc[0]['pairs'][0],df.iloc[0]['pairs'][1])])) #len(list(nx.common_neighbors(G,6,840))) # Your Answer Here


# In[41]:


new_connections_predictions()


# In[ ]:





# In[59]:





# In[ ]:




